# é˜¶æ®µå…­ï¼šé›†æˆæµ‹è¯•å’Œéƒ¨ç½²ä¼˜åŒ–

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯SQLæ ¸éªŒæœåŠ¡ç³»ç»Ÿçš„æœ€åé˜¶æ®µï¼Œä¸»è¦å®Œæˆæ•´ä¸ªç³»ç»Ÿçš„æµ‹è¯•ã€éƒ¨ç½²é…ç½®å’Œè¿ç»´ä¼˜åŒ–ã€‚æ­¤é˜¶æ®µå°†ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§ã€å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œä¸ºç”Ÿäº§ç¯å¢ƒéƒ¨ç½²åšå¥½å‡†å¤‡ã€‚

åœ¨å‰äº”ä¸ªé˜¶æ®µå®Œæˆåï¼Œç³»ç»Ÿå·²å…·å¤‡å®Œæ•´çš„åŠŸèƒ½ï¼š
- FastAPI WebæœåŠ¡æä¾›HTTPæ¥å£
- Celery Workerå¤„ç†åå°ä»»åŠ¡
- å®Œæ•´çš„SQLæ ¸éªŒä¸šåŠ¡æµç¨‹
- æ•°æ®åº“å­˜å‚¨å’ŒNFSæ–‡ä»¶ç®¡ç†

## å‰ç½®çŠ¶æ€ï¼ˆå‰äº”é˜¶æ®µå·²å®Œæˆï¼‰

### å·²å®Œæˆçš„å®Œæ•´ç³»ç»Ÿ
- âœ… é¡¹ç›®åŸºç¡€æ¶æ„ï¼ˆé…ç½®ã€æ—¥å¿—ã€å¼‚å¸¸å¤„ç†ã€å·¥å…·ç±»ï¼‰
- âœ… æ•°æ®åº“å±‚ï¼ˆè¿æ¥ç®¡ç†ã€æ•°æ®æ¨¡å‹ã€è¿ç§»ç³»ç»Ÿï¼‰
- âœ… ä¸šåŠ¡é€»è¾‘å±‚ï¼ˆJobæœåŠ¡ã€TaskæœåŠ¡ã€SQLFluffé›†æˆã€æ–‡ä»¶å¤„ç†ï¼‰
- âœ… FastAPI WebæœåŠ¡ï¼ˆAPIæ¥å£ã€ä¾èµ–æ³¨å…¥ã€Consulæ³¨å†Œï¼‰
- âœ… Celery Workerï¼ˆZIPè§£å‹ã€SQLåˆ†æã€ä»»åŠ¡ç®¡ç†ï¼‰
- âœ… å®Œæ•´çš„å¼‚æ­¥å¤„ç†æµç¨‹ï¼ˆWebæœåŠ¡ â†’ Redis â†’ Worker â†’ ç»“æœå­˜å‚¨ï¼‰

## æœ¬é˜¶æ®µç›®æ ‡
å®Œæˆç³»ç»Ÿçš„æµ‹è¯•éªŒè¯ã€éƒ¨ç½²é…ç½®ã€ç›‘æ§å‘Šè­¦å’Œè¿ç»´æ–‡æ¡£ï¼Œç¡®ä¿ç³»ç»Ÿå¯ä»¥ç¨³å®šè¿è¡Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ã€‚

## æœ¬é˜¶æ®µä»»åŠ¡æ¸…å•

### ä»»åŠ¡6.1ï¼šå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
**ç›®æ ‡**ï¼šå»ºç«‹å®Œæ•´çš„æµ‹è¯•ä½“ç³»ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§

**å…·ä½“å·¥ä½œ**ï¼š
1. åˆ›å»ºæµ‹è¯•åŸºç¡€è®¾æ–½ï¼š

```python
# tests/conftest.py - æµ‹è¯•é…ç½®å’Œfixtures
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from fastapi.testclient import TestClient
from app.core.database import Base, get_db
from app.web_main import app
import tempfile
import os

# æµ‹è¯•æ•°æ®åº“
SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

@pytest.fixture(scope="session")
def test_db():
    """åˆ›å»ºæµ‹è¯•æ•°æ®åº“"""
    Base.metadata.create_all(bind=engine)
    yield
    Base.metadata.drop_all(bind=engine)

@pytest.fixture
def db_session(test_db):
    """æ•°æ®åº“ä¼šè¯fixture"""
    session = TestingSessionLocal()
    try:
        yield session
    finally:
        session.close()

@pytest.fixture
def client(db_session):
    """æµ‹è¯•å®¢æˆ·ç«¯"""
    def override_get_db():
        try:
            yield db_session
        finally:
            db_session.close()
    
    app.dependency_overrides[get_db] = override_get_db
    with TestClient(app) as test_client:
        yield test_client
```

2. åˆ›å»ºä¸šåŠ¡é€»è¾‘å±‚å•å…ƒæµ‹è¯•ï¼š

```python
# tests/services/test_job_service.py
import pytest
from app.services.job_service import JobService
from app.schemas.job import JobCreateRequest

class TestJobService:
    def test_create_single_sql_job(self, db_session):
        """æµ‹è¯•åˆ›å»ºå•SQLå·¥ä½œ"""
        job_service = JobService(db_session)
        request = JobCreateRequest(sql_content="SELECT * FROM users;")
        
        job_id = await job_service.create_job(request)
        
        assert job_id is not None
        job = await job_service.get_job_by_id(job_id)
        assert job.submission_type == "SINGLE_FILE"
        assert job.status == "ACCEPTED"
    
    def test_create_zip_job(self, db_session):
        """æµ‹è¯•åˆ›å»ºZIPå·¥ä½œ"""
        job_service = JobService(db_session)
        request = JobCreateRequest(zip_file_path="test.zip")
        
        job_id = await job_service.create_job(request)
        
        job = await job_service.get_job_by_id(job_id)
        assert job.submission_type == "ZIP_ARCHIVE"
        assert job.status == "ACCEPTED"
```

3. åˆ›å»ºAPIé›†æˆæµ‹è¯•ï¼š

```python
# tests/api/test_jobs.py
import pytest
from fastapi.testclient import TestClient

class TestJobsAPI:
    def test_create_job_with_sql_content(self, client):
        """æµ‹è¯•åˆ›å»ºSQLå·¥ä½œAPI"""
        response = client.post(
            "/api/v1/jobs",
            json={"sql_content": "SELECT * FROM users;"}
        )
        
        assert response.status_code == 202
        data = response.json()
        assert "job_id" in data
        
    def test_get_job_status(self, client):
        """æµ‹è¯•æŸ¥è¯¢å·¥ä½œçŠ¶æ€API"""
        # å…ˆåˆ›å»ºä¸€ä¸ªå·¥ä½œ
        create_response = client.post(
            "/api/v1/jobs",
            json={"sql_content": "SELECT * FROM users;"}
        )
        job_id = create_response.json()["job_id"]
        
        # æŸ¥è¯¢å·¥ä½œçŠ¶æ€
        response = client.get(f"/api/v1/jobs/{job_id}")
        
        assert response.status_code == 200
        data = response.json()
        assert data["job_id"] == job_id
        assert "job_status" in data
```

4. åˆ›å»ºCeleryä»»åŠ¡æµ‹è¯•ï¼š

```python
# tests/celery_app/test_tasks.py
import pytest
from unittest.mock import patch, MagicMock
from app.celery_app.tasks import process_sql_file

class TestCeleryTasks:
    @patch('app.celery_app.tasks.SQLFluffService')
    @patch('app.celery_app.tasks.FileUtils')
    def test_process_sql_file_success(self, mock_file_utils, mock_sqlfluff):
        """æµ‹è¯•SQLæ–‡ä»¶å¤„ç†ä»»åŠ¡æˆåŠŸ"""
        # è®¾ç½®mock
        mock_sqlfluff.return_value.analyze_sql_file.return_value = {
            "violations": [],
            "summary": {"error_count": 0}
        }
        
        # æ‰§è¡Œä»»åŠ¡
        result = process_sql_file("test-task-id")
        
        # éªŒè¯ç»“æœ
        assert result is not None
        mock_sqlfluff.return_value.analyze_sql_file.assert_called_once()
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%ä»¥ä¸Š
- æ‰€æœ‰APIæ¥å£éƒ½æœ‰å¯¹åº”çš„é›†æˆæµ‹è¯•
- Celeryä»»åŠ¡æœ‰å®Œæ•´çš„å•å…ƒæµ‹è¯•
- æµ‹è¯•å¯ä»¥è‡ªåŠ¨åŒ–è¿è¡Œï¼Œå¹¶ç”ŸæˆæŠ¥å‘Š

### ä»»åŠ¡6.2ï¼šå¯åŠ¨è„šæœ¬å’Œéƒ¨ç½²é…ç½®
**ç›®æ ‡**ï¼šåˆ›å»ºå®Œæ•´çš„å¯åŠ¨è„šæœ¬å’Œéƒ¨ç½²é…ç½®æ–‡ä»¶

**å…·ä½“å·¥ä½œ**ï¼š
1. åˆ›å»ºWebæœåŠ¡å¯åŠ¨è„šæœ¬ï¼š

```bash
#!/bin/bash
# scripts/start_web.sh - WebæœåŠ¡å¯åŠ¨è„šæœ¬

set -e

# ç¯å¢ƒå˜é‡æ£€æŸ¥
check_env_vars() {
    local required_vars=(
        "DATABASE_URL"
        "CELERY_BROKER_URL"
        "NFS_SHARE_ROOT_PATH"
        "CONSUL_HOST"
        "CONSUL_PORT"
    )
    
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            echo "Error: Environment variable $var is not set"
            exit 1
        fi
    done
}

# æ•°æ®åº“è¿ç§»
run_migrations() {
    echo "Running database migrations..."
    alembic upgrade head
}

# å¯åŠ¨WebæœåŠ¡
start_web_service() {
    echo "Starting FastAPI web service..."
    
    # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨gunicorn
    if [[ "${ENVIRONMENT}" == "production" ]]; then
        gunicorn app.web_main:app \
            -w ${GUNICORN_WORKERS:-4} \
            -k uvicorn.workers.UvicornWorker \
            --bind 0.0.0.0:${PORT:-8000} \
            --access-logfile - \
            --error-logfile - \
            --log-level info
    else
        # å¼€å‘ç¯å¢ƒä½¿ç”¨uvicorn
        uvicorn app.web_main:app \
            --host 0.0.0.0 \
            --port ${PORT:-8000} \
            --reload
    fi
}

main() {
    echo "Starting SQL Linting Web Service..."
    check_env_vars
    run_migrations
    start_web_service
}

main "$@"
```

2. åˆ›å»ºWorkeræœåŠ¡å¯åŠ¨è„šæœ¬ï¼š

```bash
#!/bin/bash
# scripts/start_worker.sh - WorkeræœåŠ¡å¯åŠ¨è„šæœ¬

set -e

# ç¯å¢ƒå˜é‡æ£€æŸ¥
check_env_vars() {
    local required_vars=(
        "DATABASE_URL"
        "CELERY_BROKER_URL"
        "NFS_SHARE_ROOT_PATH"
    )
    
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            echo "Error: Environment variable $var is not set"
            exit 1
        fi
    done
}

# å¯åŠ¨WorkeræœåŠ¡
start_worker_service() {
    echo "Starting Celery Worker service..."
    
    celery -A app.celery_app.celery_main worker \
        --loglevel=${CELERY_LOG_LEVEL:-INFO} \
        --concurrency=${CELERY_WORKER_CONCURRENCY:-4} \
        --hostname=worker@%h \
        --max-tasks-per-child=1000 \
        --prefetch-multiplier=1
}

main() {
    echo "Starting SQL Linting Worker Service..."
    check_env_vars
    start_worker_service
}

main "$@"
```

3. åˆ›å»ºDockeré…ç½®æ–‡ä»¶ï¼š

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    pkg-config \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

# æš´éœ²ç«¯å£
EXPOSE 8000

# é»˜è®¤å‘½ä»¤
CMD ["./scripts/start_web.sh"]
```

4. åˆ›å»ºdocker-composeé…ç½®ï¼š

```yaml
# docker-compose.yml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql+pymysql://user:password@mysql:3306/sqlfluff
      - CELERY_BROKER_URL=redis://redis:6379/0
      - NFS_SHARE_ROOT_PATH=/mnt/nfs_share
    volumes:
      - nfs_share:/mnt/nfs_share
    depends_on:
      - mysql
      - redis
    command: ./scripts/start_web.sh

  worker:
    build: .
    environment:
      - DATABASE_URL=mysql+pymysql://user:password@mysql:3306/sqlfluff
      - CELERY_BROKER_URL=redis://redis:6379/0
      - NFS_SHARE_ROOT_PATH=/mnt/nfs_share
    volumes:
      - nfs_share:/mnt/nfs_share
    depends_on:
      - mysql
      - redis
    command: ./scripts/start_worker.sh

  mysql:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=sqlfluff
      - MYSQL_USER=user
      - MYSQL_PASSWORD=password
    volumes:
      - mysql_data:/var/lib/mysql

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  mysql_data:
  redis_data:
  nfs_share:
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- å¯åŠ¨è„šæœ¬å¯ä»¥æ­£å¸¸å¯åŠ¨æœåŠ¡
- Dockeré…ç½®å¯ä»¥æˆåŠŸæ„å»ºå’Œè¿è¡Œ
- docker-composeå¯ä»¥å¯åŠ¨å®Œæ•´çš„æœåŠ¡æ ˆ
- æ‰€æœ‰é…ç½®æ–‡ä»¶éƒ½æœ‰è¯¦ç»†çš„æ³¨é‡Šå’Œè¯´æ˜

### ä»»åŠ¡6.3ï¼šç›‘æ§å’Œæ—¥å¿—ä¼˜åŒ–
**ç›®æ ‡**ï¼šå®Œå–„ç›‘æ§æŒ‡æ ‡å’Œæ—¥å¿—èšåˆç³»ç»Ÿ

**å…·ä½“å·¥ä½œ**ï¼š
1. é›†æˆPrometheusç›‘æ§ï¼š

```python
# app/core/metrics.py - ç›‘æ§æŒ‡æ ‡
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
job_counter = Counter('sql_linting_jobs_total', 'Total number of jobs', ['status'])
task_counter = Counter('sql_linting_tasks_total', 'Total number of tasks', ['status'])
request_duration = Histogram('sql_linting_request_duration_seconds', 'Request duration')
active_jobs = Gauge('sql_linting_active_jobs', 'Number of active jobs')

class MetricsMiddleware:
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
        
        start_time = time.time()
        
        # æ‰§è¡Œè¯·æ±‚
        await self.app(scope, receive, send)
        
        # è®°å½•è¯·æ±‚æ—¶é—´
        duration = time.time() - start_time
        request_duration.observe(duration)

def start_metrics_server(port: int = 8001):
    """å¯åŠ¨ç›‘æ§æŒ‡æ ‡æœåŠ¡å™¨"""
    start_http_server(port)
```

2. **Celery Workerç›‘æ§API**ï¼š

```python
# app/api/routes/monitoring.py - Workerç›‘æ§æ¥å£
from fastapi import APIRouter, Depends, HTTPException, status
from typing import Dict, Any, List
from datetime import datetime
import redis
import psutil
import os

from app.celery_app.celery_main import celery_app
from app.core.logging import api_logger
from app.config.settings import get_settings

router = APIRouter()
settings = get_settings()

@router.get("/monitoring/workers")
async def get_worker_status():
    """
    è·å–æ‰€æœ‰Workerçš„çŠ¶æ€ä¿¡æ¯
    
    è¿”å›æ´»è·ƒWorkeråˆ—è¡¨ã€çŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯
    """
    try:
        # ä½¿ç”¨Celery inspectè·å–Workerä¿¡æ¯
        inspect = celery_app.control.inspect()
        
        # è·å–æ´»è·ƒWorkeråˆ—è¡¨
        active_workers = inspect.active()
        registered_tasks = inspect.registered()
        worker_stats = inspect.stats()
        
        # ç»„è£…Workerä¿¡æ¯
        workers_info = []
        
        if active_workers:
            for worker_name, tasks in active_workers.items():
                worker_info = {
                    "worker_name": worker_name,
                    "status": "online",
                    "active_tasks": len(tasks),
                    "current_tasks": [
                        {
                            "task_id": task["id"],
                            "task_name": task["name"],
                            "args": task.get("args", []),
                            "kwargs": task.get("kwargs", {}),
                            "time_start": task.get("time_start")
                        }
                        for task in tasks
                    ]
                }
                
                # æ·»åŠ Workerç»Ÿè®¡ä¿¡æ¯
                if worker_stats and worker_name in worker_stats:
                    stats = worker_stats[worker_name]
                    worker_info.update({
                        "total_tasks": stats.get("total", 0),
                        "pool_implementation": stats.get("pool", {}).get("implementation"),
                        "pool_processes": stats.get("pool", {}).get("processes"),
                        "rusage": stats.get("rusage", {})
                    })
                
                # æ·»åŠ æ³¨å†Œä»»åŠ¡ä¿¡æ¯
                if registered_tasks and worker_name in registered_tasks:
                    worker_info["registered_tasks"] = registered_tasks[worker_name]
                
                workers_info.append(worker_info)
        
        # è·å–é˜Ÿåˆ—ä¿¡æ¯
        queue_info = await get_queue_info()
        
        response = {
            "timestamp": datetime.utcnow().isoformat(),
            "total_workers": len(workers_info),
            "online_workers": len([w for w in workers_info if w["status"] == "online"]),
            "workers": workers_info,
            "queues": queue_info
        }
        
        api_logger.debug(f"WorkerçŠ¶æ€æŸ¥è¯¢æˆåŠŸ: {len(workers_info)}ä¸ªWorkeråœ¨çº¿")
        return response
        
    except Exception as e:
        api_logger.error(f"è·å–WorkerçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–WorkerçŠ¶æ€å¤±è´¥: {str(e)}"
        )

@router.get("/monitoring/queues")
async def get_queue_status():
    """
    è·å–Celeryé˜Ÿåˆ—çŠ¶æ€ä¿¡æ¯
    
    è¿”å›å„ä¸ªé˜Ÿåˆ—çš„ä»»åŠ¡æ•°é‡å’ŒçŠ¶æ€
    """
    try:
        queue_info = await get_queue_info()
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "queues": queue_info
        }
        
    except Exception as e:
        api_logger.error(f"è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {str(e)}"
        )

@router.get("/monitoring/workers/{worker_id}")
async def get_worker_detail(worker_id: str):
    """
    è·å–æŒ‡å®šWorkerçš„è¯¦ç»†ä¿¡æ¯
    
    åŒ…æ‹¬å½“å‰ä»»åŠ¡ã€èµ„æºä½¿ç”¨æƒ…å†µç­‰
    """
    try:
        inspect = celery_app.control.inspect([worker_id])
        
        # è·å–Workerè¯¦ç»†ä¿¡æ¯
        active_tasks = inspect.active()
        scheduled_tasks = inspect.scheduled()
        reserved_tasks = inspect.reserved()
        worker_stats = inspect.stats()
        
        if not active_tasks or worker_id not in active_tasks:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Worker {worker_id} ä¸å­˜åœ¨æˆ–ä¸åœ¨çº¿"
            )
        
        worker_detail = {
            "worker_id": worker_id,
            "timestamp": datetime.utcnow().isoformat(),
            "status": "online",
            "active_tasks": active_tasks.get(worker_id, []),
            "scheduled_tasks": scheduled_tasks.get(worker_id, []),
            "reserved_tasks": reserved_tasks.get(worker_id, []),
            "stats": worker_stats.get(worker_id, {}) if worker_stats else {}
        }
        
        return worker_detail
        
    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"è·å–Workerè¯¦æƒ…å¤±è´¥: {worker_id}, é”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–Workerè¯¦æƒ…å¤±è´¥: {str(e)}"
        )

@router.post("/monitoring/workers/{worker_id}/control")
async def control_worker(worker_id: str, action: str):
    """
    æ§åˆ¶Workeræ“ä½œ
    
    æ”¯æŒçš„æ“ä½œï¼šshutdown, pool_restart, reset_stats
    """
    try:
        control = celery_app.control
        
        if action == "shutdown":
            # ä¼˜é›…å…³é—­Worker
            response = control.broadcast('shutdown', destination=[worker_id])
        elif action == "pool_restart":
            # é‡å¯Workerè¿›ç¨‹æ± 
            response = control.broadcast('pool_restart', destination=[worker_id])
        elif action == "reset_stats":
            # é‡ç½®Workerç»Ÿè®¡ä¿¡æ¯
            inspect = celery_app.control.inspect([worker_id])
            response = inspect.memdump()
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ä¸æ”¯æŒçš„æ“ä½œ: {action}"
            )
        
        return {
            "worker_id": worker_id,
            "action": action,
            "timestamp": datetime.utcnow().isoformat(),
            "result": response
        }
        
    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"æ§åˆ¶Workerå¤±è´¥: {worker_id}, æ“ä½œ: {action}, é”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ§åˆ¶Workerå¤±è´¥: {str(e)}"
        )

async def get_queue_info() -> List[Dict[str, Any]]:
    """è·å–é˜Ÿåˆ—ä¿¡æ¯"""
    try:
        # è¿æ¥Redisè·å–é˜Ÿåˆ—ä¿¡æ¯
        redis_client = redis.Redis.from_url(settings.CELERY_BROKER_URL)
        
        queues = [
            "celery",           # é»˜è®¤é˜Ÿåˆ—
            "zip_processing",   # ZIPå¤„ç†é˜Ÿåˆ—
            "sql_analysis"      # SQLåˆ†æé˜Ÿåˆ—
        ]
        
        queue_info = []
        for queue_name in queues:
            # è·å–é˜Ÿåˆ—é•¿åº¦
            queue_length = redis_client.llen(queue_name)
            
            queue_info.append({
                "name": queue_name,
                "length": queue_length,
                "status": "active" if queue_length >= 0 else "inactive"
            })
        
        return queue_info
        
    except Exception as e:
        api_logger.error(f"è·å–é˜Ÿåˆ—ä¿¡æ¯å¤±è´¥: {e}")
        return []

@router.get("/monitoring/system")
async def get_system_metrics():
    """
    è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
    
    è¿”å›CPUã€å†…å­˜ã€ç£ç›˜ç­‰èµ„æºä½¿ç”¨æƒ…å†µ
    """
    try:
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disk_usage = psutil.disk_usage('/')
        
        # NFSå­˜å‚¨ä½¿ç”¨æƒ…å†µ
        nfs_usage = None
        if os.path.exists(settings.NFS_SHARE_ROOT_PATH):
            nfs_usage = psutil.disk_usage(settings.NFS_SHARE_ROOT_PATH)
        
        # ç½‘ç»œç»Ÿè®¡
        network_io = psutil.net_io_counters()
        
        # è¿›ç¨‹ä¿¡æ¯
        current_process = psutil.Process()
        process_info = {
            "pid": current_process.pid,
            "cpu_percent": current_process.cpu_percent(),
            "memory_info": current_process.memory_info()._asdict(),
            "create_time": current_process.create_time()
        }
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "cpu": {
                "usage_percent": cpu_percent,
                "core_count": cpu_count
            },
            "memory": {
                "total": memory.total,
                "available": memory.available,
                "used": memory.used,
                "usage_percent": memory.percent
            },
            "disk": {
                "total": disk_usage.total,
                "used": disk_usage.used,
                "free": disk_usage.free,
                "usage_percent": (disk_usage.used / disk_usage.total) * 100
            },
            "nfs_storage": {
                "total": nfs_usage.total if nfs_usage else 0,
                "used": nfs_usage.used if nfs_usage else 0,
                "free": nfs_usage.free if nfs_usage else 0,
                "usage_percent": (nfs_usage.used / nfs_usage.total) * 100 if nfs_usage else 0
            } if nfs_usage else None,
            "network": {
                "bytes_sent": network_io.bytes_sent,
                "bytes_recv": network_io.bytes_recv,
                "packets_sent": network_io.packets_sent,
                "packets_recv": network_io.packets_recv
            },
            "process": process_info
        }
        
    except Exception as e:
        api_logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {str(e)}"
        )
```

3. **Workerå¥åº·æ£€æŸ¥å¢å¼º**ï¼š

```python
# app/api/routes/health.py - å¢å¼ºWorkerå¥åº·æ£€æŸ¥
@router.get("/health/workers")
async def workers_health_check():
    """
    WorkeræœåŠ¡å¥åº·æ£€æŸ¥
    
    æ£€æŸ¥WorkeræœåŠ¡çš„å¯ç”¨æ€§å’Œå¥åº·çŠ¶æ€
    """
    try:
        health_status = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "service": "celery-workers",
            "checks": {}
        }
        
        # æ£€æŸ¥Celeryè¿æ¥
        try:
            inspect = celery_app.control.inspect()
            active_workers = inspect.active()
            
            if active_workers:
                worker_count = len(active_workers)
                health_status["checks"]["celery_workers"] = {
                    "status": "healthy",
                    "message": f"{worker_count}ä¸ªWorkeråœ¨çº¿",
                    "worker_count": worker_count,
                    "workers": list(active_workers.keys())
                }
            else:
                health_status["checks"]["celery_workers"] = {
                    "status": "unhealthy",
                    "message": "æ²¡æœ‰å¯ç”¨çš„Worker",
                    "worker_count": 0
                }
                health_status["status"] = "unhealthy"
                
        except Exception as e:
            health_status["checks"]["celery_workers"] = {
                "status": "unhealthy",
                "message": f"Celeryè¿æ¥å¤±è´¥: {str(e)}"
            }
            health_status["status"] = "unhealthy"
        
        # æ£€æŸ¥æ¶ˆæ¯é˜Ÿåˆ—
        try:
            redis_client = redis.Redis.from_url(settings.CELERY_BROKER_URL)
            redis_client.ping()
            
            # æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦
            queue_lengths = {}
            for queue_name in ["celery", "zip_processing", "sql_analysis"]:
                queue_lengths[queue_name] = redis_client.llen(queue_name)
            
            health_status["checks"]["message_queues"] = {
                "status": "healthy",
                "message": "æ¶ˆæ¯é˜Ÿåˆ—æ­£å¸¸",
                "queue_lengths": queue_lengths
            }
            
        except Exception as e:
            health_status["checks"]["message_queues"] = {
                "status": "unhealthy",
                "message": f"æ¶ˆæ¯é˜Ÿåˆ—è¿æ¥å¤±è´¥: {str(e)}"
            }
            health_status["status"] = "unhealthy"
        
        # æ£€æŸ¥ä»»åŠ¡å¤„ç†èƒ½åŠ›
        try:
            # å¯ä»¥å‘é€ä¸€ä¸ªæµ‹è¯•ä»»åŠ¡æ¥éªŒè¯å¤„ç†èƒ½åŠ›
            from app.celery_app.tasks import celery_app
            
            # è·å–ä»»åŠ¡ç»Ÿè®¡
            inspect = celery_app.control.inspect()
            stats = inspect.stats()
            
            total_processed = 0
            if stats:
                for worker_stats in stats.values():
                    total_processed += worker_stats.get('total', 0)
            
            health_status["checks"]["task_processing"] = {
                "status": "healthy",
                "message": "ä»»åŠ¡å¤„ç†èƒ½åŠ›æ­£å¸¸",
                "total_tasks_processed": total_processed
            }
            
        except Exception as e:
            health_status["checks"]["task_processing"] = {
                "status": "warning",
                "message": f"ä»»åŠ¡å¤„ç†èƒ½åŠ›æ£€æŸ¥å¤±è´¥: {str(e)}"
            }
        
        return health_status
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "service": "celery-workers",
            "error": str(e)
        }
```

4. é…ç½®ç»“æ„åŒ–æ—¥å¿—ï¼š

```python
# app/core/logging.py - å¢å¼ºæ—¥å¿—é…ç½®
import logging
import sys
from datetime import datetime
import json
from app.core.config import settings

class JSONFormatter(logging.Formatter):
    """JSONæ ¼å¼åŒ–å™¨"""
    
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # æ·»åŠ é¢å¤–å­—æ®µ
        if hasattr(record, 'job_id'):
            log_entry['job_id'] = record.job_id
        if hasattr(record, 'task_id'):
            log_entry['task_id'] = record.task_id
        if hasattr(record, 'user_id'):
            log_entry['user_id'] = record.user_id
            
        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)
            
        return json.dumps(log_entry)

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    
    # æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.LOG_LEVEL.upper()))
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(JSONFormatter())
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if settings.LOG_FILE:
        file_handler = logging.FileHandler(settings.LOG_FILE)
        file_handler.setFormatter(JSONFormatter())
        root_logger.addHandler(file_handler)
```

5. **åœ¨FastAPIä¸»åº”ç”¨ä¸­æ³¨å†Œç›‘æ§è·¯ç”±**ï¼š

```python
# app/web_main.py - æ³¨å†Œç›‘æ§è·¯ç”±
from app.api.routes import jobs, tasks, health, monitoring

app.include_router(jobs.router, prefix="/api/v1", tags=["jobs"])
app.include_router(tasks.router, prefix="/api/v1", tags=["tasks"])
app.include_router(health.router, prefix="/api/v1", tags=["health"])
app.include_router(monitoring.router, prefix="/api/v1", tags=["monitoring"])
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- Prometheusç›‘æ§æŒ‡æ ‡æ­£å¸¸æ”¶é›†
- Workerç›‘æ§APIå¯ä»¥è·å–å®æ—¶WorkerçŠ¶æ€
- é˜Ÿåˆ—ç›‘æ§APIå¯ä»¥æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—æƒ…å†µ
- Workerå¥åº·æ£€æŸ¥è¦†ç›–æ‰€æœ‰å…³é”®åŠŸèƒ½
- æ—¥å¿—æ ¼å¼ç»Ÿä¸€ï¼Œä¾¿äºèšåˆå’Œåˆ†æ
- ç›‘æ§æŒ‡æ ‡å¯ä»¥ç”¨äºå‘Šè­¦å’Œå¯è§†åŒ–

### 6. **ç›‘æ§ä»ªè¡¨æ¿é…ç½®**ï¼š

```python
# monitoring_dashboard.py - ç®€å•çš„ç›‘æ§ä»ªè¡¨æ¿
import asyncio
import aiohttp
import json
from datetime import datetime
import time

class WorkerMonitoringDashboard:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    async def get_worker_status(self):
        """è·å–WorkerçŠ¶æ€"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/api/v1/monitoring/workers") as response:
                return await response.json()
    
    async def get_system_metrics(self):
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/api/v1/monitoring/system") as response:
                return await response.json()
    
    async def get_health_status(self):
        """è·å–å¥åº·çŠ¶æ€"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/api/v1/health/workers") as response:
                return await response.json()
    
    def print_dashboard(self, worker_status, system_metrics, health_status):
        """æ‰“å°ä»ªè¡¨æ¿"""
        print("\n" + "="*60)
        print(f"SQL Linting Service - Worker Dashboard")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        # WorkerçŠ¶æ€
        print(f"\nğŸ“Š Worker Status:")
        print(f"  Total Workers: {worker_status.get('total_workers', 0)}")
        print(f"  Online Workers: {worker_status.get('online_workers', 0)}")
        
        for worker in worker_status.get('workers', []):
            print(f"  â€¢ {worker['worker_name']}: {worker['active_tasks']} active tasks")
        
        # é˜Ÿåˆ—çŠ¶æ€
        print(f"\nğŸ“¦ Queue Status:")
        for queue in worker_status.get('queues', []):
            print(f"  â€¢ {queue['name']}: {queue['length']} tasks ({queue['status']})")
        
        # ç³»ç»Ÿèµ„æº
        print(f"\nğŸ’» System Resources:")
        cpu = system_metrics.get('cpu', {})
        memory = system_metrics.get('memory', {})
        print(f"  CPU: {cpu.get('usage_percent', 0):.1f}% ({cpu.get('core_count', 0)} cores)")
        print(f"  Memory: {memory.get('usage_percent', 0):.1f}% ({memory.get('used', 0)//1048576}MB used)")
        
        # å¥åº·çŠ¶æ€
        print(f"\nğŸ¥ Health Status:")
        overall_status = health_status.get('status', 'unknown')
        print(f"  Overall: {overall_status.upper()}")
        
        for check_name, check_info in health_status.get('checks', {}).items():
            status = check_info.get('status', 'unknown')
            message = check_info.get('message', '')
            print(f"  â€¢ {check_name}: {status.upper()} - {message}")
        
        print("="*60)
    
    async def run_dashboard(self, refresh_interval=30):
        """è¿è¡Œä»ªè¡¨æ¿"""
        while True:
            try:
                # è·å–æ‰€æœ‰ç›‘æ§æ•°æ®
                worker_status = await self.get_worker_status()
                system_metrics = await self.get_system_metrics()
                health_status = await self.get_health_status()
                
                # æ¸…å±å¹¶æ‰“å°ä»ªè¡¨æ¿
                import os
                os.system('clear' if os.name == 'posix' else 'cls')
                self.print_dashboard(worker_status, system_metrics, health_status)
                
                # ç­‰å¾…åˆ·æ–°é—´éš”
                await asyncio.sleep(refresh_interval)
                
            except Exception as e:
                print(f"Error updating dashboard: {e}")
                await asyncio.sleep(refresh_interval)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    dashboard = WorkerMonitoringDashboard()
    await dashboard.run_dashboard(refresh_interval=10)

if __name__ == "__main__":
    asyncio.run(main())
```

ä½¿ç”¨æ–¹æ³•ï¼š
```bash
# å¯åŠ¨ç›‘æ§ä»ªè¡¨æ¿
python monitoring_dashboard.py
```

### 7. **Grafanaé›†æˆé…ç½®**ï¼š

```yaml
# grafana-dashboard.json - Grafanaä»ªè¡¨æ¿é…ç½®
{
  "dashboard": {
    "title": "SQL Linting Service - Worker Monitoring",
    "panels": [
      {
        "title": "Worker Status",
        "type": "stat",
        "targets": [
          {
            "url": "http://localhost:8000/api/v1/monitoring/workers",
            "jsonPath": "$.online_workers"
          }
        ]
      },
      {
        "title": "Queue Length",
        "type": "graph",
        "targets": [
          {
            "url": "http://localhost:8000/api/v1/monitoring/queues",
            "jsonPath": "$.queues[*].length"
          }
        ]
      },
      {
        "title": "System Resources",
        "type": "graph",
        "targets": [
          {
            "url": "http://localhost:8000/api/v1/monitoring/system",
            "jsonPath": "$.cpu.usage_percent"
          }
        ]
      }
    ]
  }
}
```

### ä»»åŠ¡6.4ï¼šéƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æŒ‡å—
**ç›®æ ‡**ï¼šç¼–å†™å®Œæ•´çš„éƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æŒ‡å—

**å…·ä½“å·¥ä½œ**ï¼š
1. åˆ›å»ºéƒ¨ç½²æŒ‡å—ï¼š

```markdown
# éƒ¨ç½²æŒ‡å—

## ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **WebæœåŠ¡å™¨**: CPU 2æ ¸+, å†…å­˜ 4GB+, ç£ç›˜ 20GB+
- **WorkeræœåŠ¡å™¨**: CPU 4æ ¸+, å†…å­˜ 8GB+, ç£ç›˜ 50GB+
- **æ•°æ®åº“æœåŠ¡å™¨**: CPU 2æ ¸+, å†…å­˜ 4GB+, ç£ç›˜ 100GB+

### è½¯ä»¶è¦æ±‚
- Python 3.11+
- MySQL 8.0+
- Redis 6.0+
- NFSæœåŠ¡å™¨
- Consul 1.15+

## éƒ¨ç½²æ­¥éª¤

### 1. å‡†å¤‡ç¯å¢ƒ
```bash
# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
export DATABASE_URL="mysql+pymysql://user:password@host:port/database"
export CELERY_BROKER_URL="redis://host:port/0"
export NFS_SHARE_ROOT_PATH="/mnt/nfs_share"
```

### 2. æ•°æ®åº“åˆå§‹åŒ–
```bash
# è¿è¡Œæ•°æ®åº“è¿ç§»
alembic upgrade head
```

### 3. å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨WebæœåŠ¡
./scripts/start_web.sh

# å¯åŠ¨WorkeræœåŠ¡
./scripts/start_worker.sh
```

## è¿ç»´ç›‘æ§

### ç›‘æ§æŒ‡æ ‡
- è¯·æ±‚å“åº”æ—¶é—´
- ä»»åŠ¡å¤„ç†é€Ÿåº¦
- é”™è¯¯ç‡
- èµ„æºä½¿ç”¨æƒ…å†µ

### Workerç›‘æ§API
ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„Workerç›‘æ§APIï¼Œç”¨äºå®æ—¶ç›‘æ§Celery Workerçš„çŠ¶æ€å’Œæ€§èƒ½ï¼š

#### 1. è·å–æ‰€æœ‰WorkerçŠ¶æ€
```bash
curl http://localhost:8000/api/v1/monitoring/workers
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "timestamp": "2025-01-27T10:30:00.123456",
  "total_workers": 2,
  "online_workers": 2,
  "workers": [
    {
      "worker_name": "worker@server1",
      "status": "online",
      "active_tasks": 3,
      "total_tasks": 150,
      "pool_processes": 4,
      "current_tasks": [
        {
          "task_id": "abc123",
          "task_name": "app.celery_app.tasks.process_sql_file",
          "time_start": 1640000000.0
        }
      ]
    }
  ],
  "queues": [
    {
      "name": "sql_analysis",
      "length": 5,
      "status": "active"
    }
  ]
}
```

#### 2. è·å–ç‰¹å®šWorkerè¯¦æƒ…
```bash
curl http://localhost:8000/api/v1/monitoring/workers/worker@server1
```

#### 3. è·å–é˜Ÿåˆ—çŠ¶æ€
```bash
curl http://localhost:8000/api/v1/monitoring/queues
```

#### 4. è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
```bash
curl http://localhost:8000/api/v1/monitoring/system
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "timestamp": "2025-01-27T10:30:00.123456",
  "cpu": {
    "usage_percent": 45.2,
    "core_count": 8
  },
  "memory": {
    "total": 8589934592,
    "available": 4294967296,
    "used": 4294967296,
    "usage_percent": 50.0
  },
  "nfs_storage": {
    "total": 1073741824000,
    "used": 536870912000,
    "free": 536870912000,
    "usage_percent": 50.0
  }
}
```

#### 5. Workerå¥åº·æ£€æŸ¥
```bash
curl http://localhost:8000/api/v1/health/workers
```

### å‘Šè­¦è®¾ç½®å»ºè®®
åŸºäºç›‘æ§APIæ•°æ®ï¼Œå»ºè®®è®¾ç½®ä»¥ä¸‹å‘Šè­¦è§„åˆ™ï¼š

1. **Workerç¦»çº¿å‘Šè­¦**ï¼š
   - æ¡ä»¶ï¼šonline_workers < 1
   - ä¸¥é‡ç¨‹åº¦ï¼šCritical

2. **é˜Ÿåˆ—ç§¯å‹å‘Šè­¦**ï¼š
   - æ¡ä»¶ï¼šé˜Ÿåˆ—é•¿åº¦ > 100
   - ä¸¥é‡ç¨‹åº¦ï¼šWarning

3. **ç³»ç»Ÿèµ„æºå‘Šè­¦**ï¼š
   - CPUä½¿ç”¨ç‡ > 80%ï¼šWarning
   - å†…å­˜ä½¿ç”¨ç‡ > 85%ï¼šWarning
   - ç£ç›˜ä½¿ç”¨ç‡ > 90%ï¼šCritical

4. **ä»»åŠ¡å¤„ç†å¼‚å¸¸å‘Šè­¦**ï¼š
   - ä»»åŠ¡å¤±è´¥ç‡ > 10%ï¼šWarning
   - ä»»åŠ¡å¹³å‡å¤„ç†æ—¶é—´ > 5åˆ†é’Ÿï¼šWarning

### æ—¥å¿—ä½ç½®
- WebæœåŠ¡æ—¥å¿—: `/var/log/sqlfluff/web.log`
- Workeræ—¥å¿—: `/var/log/sqlfluff/worker.log`
```

2. åˆ›å»ºæ•…éšœæ’æŸ¥æ–‡æ¡£ï¼š

```markdown
# æ•…éšœæ’æŸ¥æŒ‡å—

## å¸¸è§é—®é¢˜

### 1. æœåŠ¡å¯åŠ¨å¤±è´¥
**ç—‡çŠ¶**: æœåŠ¡æ— æ³•å¯åŠ¨æˆ–ç«‹å³é€€å‡º
**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
2. æ£€æŸ¥ä¾èµ–æœåŠ¡çŠ¶æ€ï¼ˆMySQLã€Redisã€NFSï¼‰
3. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
4. éªŒè¯ç½‘ç»œè¿é€šæ€§

### 2. ä»»åŠ¡å¤„ç†å¤±è´¥
**ç—‡çŠ¶**: ä»»åŠ¡çŠ¶æ€ä¸€ç›´ä¸ºPENDINGæˆ–FAILURE
**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥WorkeræœåŠ¡çŠ¶æ€
2. æ£€æŸ¥Redisè¿æ¥
3. æ£€æŸ¥SQLæ–‡ä»¶æ˜¯å¦å­˜åœ¨
4. æ£€æŸ¥SQLFluffé…ç½®

### 3. Workerç›‘æ§ç›¸å…³é—®é¢˜
**ç—‡çŠ¶**: Workerç›‘æ§APIè¿”å›å¼‚å¸¸æˆ–WorkerçŠ¶æ€ä¸æ­£ç¡®
**æ’æŸ¥æ­¥éª¤**:
1. ä½¿ç”¨ç›‘æ§APIæ£€æŸ¥WorkerçŠ¶æ€ï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/workers
   ```
2. æ£€æŸ¥Workerè¿›ç¨‹æ˜¯å¦æ­£åœ¨è¿è¡Œï¼š
   ```bash
   ps aux | grep celery
   ```
3. æ£€æŸ¥Workeræ—¥å¿—ï¼š
   ```bash
   tail -f /var/log/sqlfluff/worker.log
   ```
4. æ£€æŸ¥æ¶ˆæ¯é˜Ÿåˆ—çŠ¶æ€ï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/queues
   ```
5. éªŒè¯Redisè¿æ¥ï¼š
   ```bash
   redis-cli -u $CELERY_BROKER_URL ping
   ```

### 4. é˜Ÿåˆ—ç§¯å‹é—®é¢˜
**ç—‡çŠ¶**: ä»»åŠ¡é•¿æ—¶é—´æ’é˜Ÿï¼Œå¤„ç†ç¼“æ…¢
**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦ï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/queues
   ```
2. æ£€æŸ¥Workerå¹¶å‘è®¾ç½®ï¼š
   ```bash
   # è°ƒæ•´Workerå¹¶å‘æ•°
   export CELERY_WORKER_CONCURRENCY=8
   ```
3. å¢åŠ Workerå®ä¾‹ï¼š
   ```bash
   # å¯åŠ¨é¢å¤–çš„Worker
   ./scripts/start_worker.sh &
   ```
4. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨ï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/system
   ```

### 5. Workeræ€§èƒ½é—®é¢˜
**ç—‡çŠ¶**: Worker CPUæˆ–å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
**æ’æŸ¥æ­¥éª¤**:
1. ç›‘æ§ç³»ç»Ÿèµ„æºï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/system
   ```
2. æ£€æŸ¥å•ä¸ªWorkerçš„ä»»åŠ¡è´Ÿè½½ï¼š
   ```bash
   curl http://localhost:8000/api/v1/monitoring/workers/worker@hostname
   ```
3. è°ƒæ•´Workeré…ç½®ï¼š
   ```bash
   # é™ä½å¹¶å‘æ•°
   export CELERY_WORKER_CONCURRENCY=2
   # å¢åŠ ä»»åŠ¡é‡å¯é¢‘ç‡
   export CELERY_WORKER_MAX_TASKS_PER_CHILD=500
   ```
4. æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼š
   - æŸ¥çœ‹active_tasksä¸­çš„time_startæ—¶é—´æˆ³
   - è€ƒè™‘è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´

### 6. Workerå¥åº·æ£€æŸ¥å¤±è´¥
**ç—‡çŠ¶**: Workerå¥åº·æ£€æŸ¥æ¥å£è¿”å›unhealthyçŠ¶æ€
**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥Workerå¥åº·çŠ¶æ€ï¼š
   ```bash
   curl http://localhost:8000/api/v1/health/workers
   ```
2. æ ¹æ®è¿”å›çš„checksä¿¡æ¯å®šä½å…·ä½“é—®é¢˜ï¼š
   - `celery_workers`: Workerè¿æ¥é—®é¢˜
   - `message_queues`: Redisé˜Ÿåˆ—é—®é¢˜
   - `task_processing`: ä»»åŠ¡å¤„ç†èƒ½åŠ›é—®é¢˜
3. é‡å¯ç›¸åº”çš„æœåŠ¡æˆ–ç»„ä»¶

### 7. æ€§èƒ½é—®é¢˜
**ç—‡çŠ¶**: è¯·æ±‚å“åº”æ…¢æˆ–ä»»åŠ¡å¤„ç†ç¼“æ…¢
**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
2. æ£€æŸ¥æ•°æ®åº“æ€§èƒ½
3. è°ƒæ•´Workerå¹¶å‘æ•°
4. æ£€æŸ¥NFSæ€§èƒ½
5. ä½¿ç”¨ç›‘æ§APIæŒç»­è§‚å¯Ÿæ€§èƒ½æŒ‡æ ‡
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- éƒ¨ç½²æ–‡æ¡£è¯¦ç»†å®Œæ•´ï¼Œå¯ä»¥æŒ‰æ­¥éª¤æˆåŠŸéƒ¨ç½²
- è¿ç»´æŒ‡å—æ¶µç›–ç›‘æ§ã€æ—¥å¿—ã€æ•…éšœæ’æŸ¥ç­‰å…³é”®å†…å®¹
- æ–‡æ¡£ç»“æ„æ¸…æ™°ï¼Œä¾¿äºæŸ¥é˜…å’Œç»´æŠ¤

### ä»»åŠ¡6.5ï¼šæ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
**ç›®æ ‡**ï¼šè¿›è¡Œæ€§èƒ½æµ‹è¯•å¹¶ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

**å…·ä½“å·¥ä½œ**ï¼š
1. åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬ï¼š

```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

async def create_job(session, sql_content):
    """åˆ›å»ºä¸€ä¸ªæ ¸éªŒå·¥ä½œ"""
    async with session.post(
        "http://localhost:8000/api/v1/jobs",
        json={"sql_content": sql_content}
    ) as response:
        return await response.json()

async def load_test(concurrent_requests=10, total_requests=100):
    """è´Ÿè½½æµ‹è¯•"""
    async with aiohttp.ClientSession() as session:
        sql_content = "SELECT * FROM users WHERE id = 1;"
        
        start_time = time.time()
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i in range(total_requests):
            task = create_job(session, sql_content)
            tasks.append(task)
            
            # æ§åˆ¶å¹¶å‘æ•°
            if len(tasks) >= concurrent_requests:
                await asyncio.gather(*tasks)
                tasks = []
        
        # å¤„ç†å‰©ä½™ä»»åŠ¡
        if tasks:
            await asyncio.gather(*tasks)
        
        end_time = time.time()
        
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"å¹¶å‘æ•°: {concurrent_requests}")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"QPS: {total_requests / (end_time - start_time):.2f}")

if __name__ == "__main__":
    asyncio.run(load_test())
```

2. æ€§èƒ½ä¼˜åŒ–é…ç½®ï¼š

```python
# æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
engine = create_engine(
    DATABASE_URL,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600
)

# Redisè¿æ¥æ± ä¼˜åŒ–
redis_pool = redis.ConnectionPool.from_url(
    CELERY_BROKER_URL,
    max_connections=50
)

# Celery Workerä¼˜åŒ–
celery_app.conf.update(
    worker_prefetch_multiplier=1,
    task_max_retries=3,
    worker_max_tasks_per_child=1000,
    worker_concurrency=8  # æ ¹æ®CPUæ ¸æ•°è°ƒæ•´
)
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- æ€§èƒ½æµ‹è¯•è„šæœ¬å¯ä»¥æ­£å¸¸è¿è¡Œ
- ç³»ç»Ÿå¯ä»¥æ‰¿å—é¢„æœŸçš„å¹¶å‘è´Ÿè½½
- å…³é”®æ€§èƒ½æŒ‡æ ‡æ»¡è¶³ä¸šåŠ¡è¦æ±‚
- ä¼˜åŒ–é…ç½®å·²åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ

## æœ¬é˜¶æ®µå®Œæˆæ ‡å¿—
- [ ] å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•å®Œæ•´ï¼Œè¦†ç›–ç‡è¾¾æ ‡
- [ ] å¯åŠ¨è„šæœ¬å’Œéƒ¨ç½²é…ç½®å®Œæˆå¹¶éªŒè¯
- [ ] ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿæ­£å¸¸å·¥ä½œ
- [ ] éƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æŒ‡å—ç¼–å†™å®Œæˆ
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿæ€§èƒ½æ»¡è¶³è¦æ±‚
- [ ] å®Œæ•´çš„CI/CDæµç¨‹é…ç½®ï¼ˆå¯é€‰ï¼‰
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²éªŒè¯é€šè¿‡

## é¡¹ç›®æ€»ç»“
å®Œæˆç¬¬å…­é˜¶æ®µåï¼ŒSQLæ ¸éªŒæœåŠ¡ç³»ç»Ÿå°†å®Œå…¨æ»¡è¶³ç”Ÿäº§ç¯å¢ƒçš„è¦æ±‚ï¼š

### åŠŸèƒ½å®Œæ•´æ€§
- âœ… æ”¯æŒå•SQLæ–‡ä»¶å’ŒZIPåŒ…ä¸¤ç§æäº¤æ–¹å¼
- âœ… å®Œæ•´çš„å¼‚æ­¥å¤„ç†æµç¨‹
- âœ… åŸºäºSQLFluffçš„è´¨é‡åˆ†æ
- âœ… ç»“æœæ–‡ä»¶ç®¡ç†å’ŒæŸ¥è¯¢

### ç³»ç»Ÿå¯é æ€§
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- âœ… åˆ†å¸ƒå¼éƒ¨ç½²æ”¯æŒ
- âœ… æ•°æ®åº“äº‹åŠ¡ä¸€è‡´æ€§
- âœ… èµ„æºæ¸…ç†å’Œå†…å­˜ç®¡ç†

### è¿ç»´å‹å¥½æ€§
- âœ… ç»“æ„åŒ–æ—¥å¿—å’Œç›‘æ§æŒ‡æ ‡
- âœ… å¥åº·æ£€æŸ¥å’ŒæœåŠ¡å‘ç°
- âœ… å®Œæ•´çš„éƒ¨ç½²æ–‡æ¡£
- âœ… æ•…éšœæ’æŸ¥æŒ‡å—

### æ€§èƒ½å¯æ‰©å±•æ€§
- âœ… æ°´å¹³æ‰©å±•æ”¯æŒ
- âœ… è¿æ¥æ± å’Œç¼“å­˜ä¼˜åŒ–
- âœ… è´Ÿè½½å‡è¡¡å…¼å®¹æ€§
- âœ… æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

æ•´ä¸ªé¡¹ç›®å·²ç»å¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œä¸ºå®¢æˆ·æä¾›ç¨³å®šå¯é çš„SQLæ ¸éªŒæœåŠ¡ã€‚ 